# üêõ Bug Report ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç - train_model_v2.py

## ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
‚úÖ **‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞**: ‡∏ó‡∏∏‡∏Å bugs ‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß - ‡πÇ‡∏Ñ‡πâ‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!

---

## üîç Bugs ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç

### **Bug #1: PurgedGroupTimeSeriesSplit - Logic Error** üî¥ CRITICAL
**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:**
- Logic ‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á train/test sets ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
- `purge_start` ‡πÅ‡∏•‡∏∞ `purge_end` ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
- Test set ‡πÉ‡∏ä‡πâ `indices[test_end:]` ‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏°‡πà‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠

**‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö:**
- Cross-validation ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à
- ‡∏≠‡∏≤‡∏à‡∏°‡∏µ data leakage ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á train ‡πÅ‡∏•‡∏∞ test
- ‡∏ú‡∏•‡∏Å‡∏≤‡∏£ validation ‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠

**‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:**
```python
# ‡πÄ‡∏î‡∏¥‡∏° (‡∏ú‡∏¥‡∏î):
purge_start = train_end - gaps
purge_end = test_start + gaps
train_indices = indices[train_start:purge_start]
test_indices = indices[test_end:] if i == self.n_splits - 1 else indices[test_start:test_end]

# ‡πÉ‡∏´‡∏°‡πà (‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á):
test_start = train_end + gaps  # ‡πÄ‡∏û‡∏¥‡πà‡∏° gap ‡∏´‡∏•‡∏±‡∏á train
test_end = min(test_start + fold_size, n_samples)
train_indices = indices[train_start:train_end]
test_indices = indices[test_start:test_end]
```

---

### **Bug #2: objective_random_forest - Parameter Conflict** üü° MEDIUM
**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:**
```python
'max_samples': trial.suggest_float('max_samples', 0.6, 1.0) if trial.suggest_categorical('use_max_samples', [True, False]) else None
```
- ‡πÉ‡∏ä‡πâ `trial.suggest_categorical` ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô conditional expression
- Optuna ‡∏à‡∏∞‡∏™‡∏∏‡πà‡∏° parameter ‡∏™‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î inconsistency

**‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö:**
- Optuna ‡∏≠‡∏≤‡∏à error ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
- Hyperparameter tuning ‡πÑ‡∏°‡πà efficient

**‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:**
```python
# ‡πÄ‡∏ä‡πá‡∏Ñ bootstrap ‡∏Å‡πà‡∏≠‡∏ô (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ max_samples ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö bootstrap=True ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
if params['bootstrap']:
    params['max_samples'] = trial.suggest_float('max_samples', 0.6, 1.0)
```

---

### **Bug #3: train_models_with_best_params - Missing num_class** üü° MEDIUM
**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:**
- LightGBM ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ `num_class` parameter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö multi-class classification
- ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏∏ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏≠‡∏≤‡∏à error ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î

**‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö:**
- LightGBM training ‡∏≠‡∏≤‡∏à fail
- ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô binary classification)

**‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:**
```python
lgb_params['num_class'] = 3  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ
lgb_params['verbose'] = -1   # ‡∏õ‡∏¥‡∏î warning
best_lgb = LGBMClassifier(**lgb_params, random_state=42, n_jobs=-1, verbose=-1)
```

---

### **Bug #4: main() - Undefined Variable** üî¥ CRITICAL
**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:**
```python
trainer.save_models(best_models, trainer.best_model_name)
```
- ‡πÑ‡∏°‡πà‡∏°‡∏µ attribute `best_model_name` ‡πÉ‡∏ô trainer
- ‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î `AttributeError` ‡∏ï‡∏≠‡∏ô‡∏£‡∏±‡∏ô

**‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö:**
- ‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° crash ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å evaluation
- Model ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å

**‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:**
```python
if results and trainer.best_model is not None:
    best_model_name = max(results, key=lambda x: results[x]['success_rate'])
    print(f"\nüíæ Saving best model ({best_model_name})...")
    trainer.save_models(best_models, best_model_name)
```

---

## ‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç

### Test Results:
```
‚úÖ [1/5] Imports - PASSED
‚úÖ [2/5] PurgedGroupTimeSeriesSplit - PASSED (3 splits, no overlap)
‚úÖ [3/5] Trainer initialization - PASSED
‚úÖ [4/5] Feature engineering - PASSED (804 rows, 44 columns)
‚úÖ [5/5] Target creation - PASSED (Features: 804x25)
```

### Target Distribution (Sample Data):
- Neutral: 803 (99.9%)
- Profit: 1 (0.1%)

**‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:** ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏õ‡∏Å‡∏ï‡∏¥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢ (1000 rows) 
‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á (98,000+ rows) ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Å‡∏ß‡πà‡∏≤

---

## üìã ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### 1. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß:
```bash
python quick_test.py
```

### 2. ‡∏£‡∏±‡∏ô Training ‡∏à‡∏£‡∏¥‡∏á:
```bash
python train_model_v2.py
```

**‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô:**
- ‡∏Å‡∏≤‡∏£ train ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ **30-60 ‡∏ô‡∏≤‡∏ó‡∏µ** (‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö CPU/GPU)
- ‡πÉ‡∏ä‡πâ RAM ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 4-8 GB
- ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå:
  - `best_trading_model.pkl` - ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
  - `feature_scaler.pkl` - Scaler ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö normalize features
  - `feature_columns.pkl` - ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á features
  - `optuna_plots/*.html` - Visualization ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£ tuning

### 3. ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Å‡∏≤‡∏£ Tuning:
```python
# ‡πÉ‡∏ô main() function:
studies, best_params = trainer.advanced_auto_tune(
    n_trials=50,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 100-200 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤
    models_to_tune=['xgboost', 'lightgbm']  # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
)
```

---

## üö® ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

### 1. Memory Usage
‡∏ñ‡πâ‡∏≤ RAM ‡πÑ‡∏°‡πà‡∏û‡∏≠ ‡πÉ‡∏´‡πâ‡∏•‡∏î:
- `n_trials` ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ tuning
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà tune (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á tune ‡∏ó‡∏±‡πâ‡∏á 4 ‡πÇ‡∏°‡πÄ‡∏î‡∏•)

### 2. Data Quality
- Target distribution ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á 3 classes (Loss, Neutral, Profit)
- ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÅ‡∏Ñ‡πà 1-2 classes ‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ß‡πà‡∏≤:
  - ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡πÑ‡∏°‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì buy ‡πÄ‡∏•‡∏¢
  - Stop loss/Take profit ‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
  - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

### 3. Computational Cost
- **XGBoost**: ‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (~5-10 ‡∏ô‡∏≤‡∏ó‡∏µ)
- **LightGBM**: ‡πÄ‡∏£‡πá‡∏ß (~5-10 ‡∏ô‡∏≤‡∏ó‡∏µ)
- **RandomForest**: ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (~10-15 ‡∏ô‡∏≤‡∏ó‡∏µ)
- **LSTM**: ‡∏ä‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (~20-30 ‡∏ô‡∏≤‡∏ó‡∏µ)

---

## üìà Expected Performance

‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• PAXG-USDT:
- **Accuracy**: 55-65%
- **F1-Score**: 0.55-0.65
- **Success Rate**: 60-70% (metric ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)

**Success Rate** = % ‡∏Ç‡∏≠‡∏á‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì buy ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏Å‡∏≥‡πÑ‡∏£‡∏à‡∏£‡∏¥‡∏á

---

## üéØ ‡∏™‡∏£‡∏∏‡∏õ

**train_model_v2.py ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß!**

‡∏ó‡∏∏‡∏Å bugs ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ñ‡∏π‡∏Å‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:
1. ‚úÖ Cross-validation ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
2. ‚úÖ Hyperparameter tuning ‡πÑ‡∏°‡πà‡∏°‡∏µ conflict
3. ‚úÖ Model training ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å algorithms
4. ‚úÖ Model saving ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

**Next Steps:**
1. ‡∏£‡∏±‡∏ô `python quick_test.py` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
2. ‡∏£‡∏±‡∏ô `python train_model_v2.py` ‡πÄ‡∏û‡∏∑‡πà‡∏≠ train ‡∏à‡∏£‡∏¥‡∏á
3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô `optuna_plots/` folder
4. ‡∏ô‡∏≥ `best_trading_model.pkl` ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á

---

**Created:** 2025-11-02  
**Status:** ‚úÖ PRODUCTION READY
